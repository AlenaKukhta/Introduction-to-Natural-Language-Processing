{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "736d8940",
   "metadata": {},
   "source": [
    "# Практическое задание к уроку 10. Машинный перевод. Модель seq2seq и механизм внимания\n",
    "\n",
    "\n",
    "Разобраться с моделькой перевода как она устроена\n",
    "запустить для перевода с русского на английский (при желании можно взять другие пары языков) два варианта с вниманием и без внимания \n",
    "\n",
    "оценить качество насколько корректно переводит (для теста отобрать примеры с увеличением длины текста) (так как оценка визуальная достаточно 20-ти примеров в тестовой выборке)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65938225",
   "metadata": {},
   "source": [
    "#### Neural machine translation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "49773a76",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as ticker\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import unicodedata\n",
    "import re\n",
    "import numpy as np\n",
    "import os\n",
    "import io\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d19951be",
   "metadata": {},
   "source": [
    "#### Download and prepare the dataset\n",
    "\n",
    "We'll use a language dataset provided by http://www.manythings.org/anki/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5e990806",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2022-06-30 13:35:42--  http://www.manythings.org/anki/rus-eng.zip\n",
      "Resolving www.manythings.org (www.manythings.org)... 173.254.30.110\n",
      "Connecting to www.manythings.org (www.manythings.org)|173.254.30.110|:80... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 14819554 (14M) [application/zip]\n",
      "Saving to: 'rus-eng.zip'\n",
      "\n",
      "rus-eng.zip         100%[===================>]  14.13M   858KB/s    in 16s     \n",
      "\n",
      "2022-06-30 13:35:59 (880 KB/s) - 'rus-eng.zip' saved [14819554/14819554]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!wget http://www.manythings.org/anki/rus-eng.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4ff04491",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Archive:  rus-eng.zip\n",
      "  inflating: rus.txt                 \n",
      "  inflating: _about.txt              \n"
     ]
    }
   ],
   "source": [
    "# !mkdir rus-eng\n",
    "# !unzip rus-eng.zip -d rus-eng/\n",
    "!unzip rus-eng.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "aa171391",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !ls /rus-eng/ -lah"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "38047d9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download the file\n",
    "# path_to_file = \"rus-eng/rus.txt\"\n",
    "path_to_file = \"rus.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9ba0d299",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_sentence(w):\n",
    "  w = w.lower().strip()\n",
    "\n",
    "  # creating a space between a word and the punctuation following it\n",
    "  # eg: \"he is a boy.\" => \"he is a boy .\"\n",
    "  # Reference:- https://stackoverflow.com/questions/3645931/python-padding-punctuation-with-white-spaces-keeping-punctuation\n",
    "  w = re.sub(r\"([?.!,])\", r\" \\1 \", w)\n",
    "  w = re.sub(r'[\" \"]+', \" \", w)\n",
    "\n",
    "  # replacing everything with space except (a-z, A-Z, \".\", \"?\", \"!\", \",\")\n",
    "  w = re.sub(r\"[^a-zA-Zа-яА-Я?.!,']+\", \" \", w)\n",
    "\n",
    "  w = w.strip()\n",
    "\n",
    "  # adding a start and an end token to the sentence\n",
    "  # so that the model know when to start and stop predicting.\n",
    "  w = '<start> ' + w + ' <end>'\n",
    "  return w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "dc330ed1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"<start> i can't go . <end>\""
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preprocess_sentence(\"I can't go.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5f335674",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Remove the accents\n",
    "# 2. Clean the sentences\n",
    "# 3. Return word pairs in the format: [ENG, RUS]\n",
    "def create_dataset(path, num_examples):\n",
    "  lines = io.open(path, encoding='UTF-8').read().strip().split('\\n')\n",
    "\n",
    "  word_pairs = [[preprocess_sentence(w) for w in l.split('\\t')[:2]]  for l in lines[:num_examples]]\n",
    "\n",
    "  return zip(*word_pairs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ab0281a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<start> go . <end>\n",
      "<start> марш ! <end>\n"
     ]
    }
   ],
   "source": [
    "en, ru = create_dataset(path_to_file, None)\n",
    "print(en[0])\n",
    "print(ru[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b67de801",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(lang):\n",
    "  lang_tokenizer = tf.keras.preprocessing.text.Tokenizer(\n",
    "      filters='')\n",
    "  lang_tokenizer.fit_on_texts(lang)\n",
    "\n",
    "  tensor = lang_tokenizer.texts_to_sequences(lang)\n",
    "\n",
    "  tensor = tf.keras.preprocessing.sequence.pad_sequences(tensor,\n",
    "                                                         padding='post')\n",
    "\n",
    "  return tensor, lang_tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4caddf8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dataset(path, num_examples=None):\n",
    "  # creating cleaned input, output pairs\n",
    "  targ_lang, inp_lang = create_dataset(path, num_examples)\n",
    "\n",
    "  input_tensor, inp_lang_tokenizer = tokenize(inp_lang)\n",
    "  target_tensor, targ_lang_tokenizer = tokenize(targ_lang)\n",
    "\n",
    "  return input_tensor, target_tensor, inp_lang_tokenizer, targ_lang_tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af843ebb",
   "metadata": {},
   "source": [
    "#### Limit the size of the dataset to experiment faster (optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e48af07b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(444587, 444587)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(en), len(ru)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "83c45dde",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try experimenting with the size of that dataset\n",
    "num_examples = 100000\n",
    "input_tensor, target_tensor, inp_lang, targ_lang = load_dataset(path_to_file, num_examples)\n",
    "\n",
    "# Calculate max_length of the target tensors\n",
    "max_length_targ, max_length_inp = target_tensor.shape[1], input_tensor.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4527cb3d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "80000 80000 20000 20000\n"
     ]
    }
   ],
   "source": [
    "# Creating training and validation sets using an 80-20 split\n",
    "input_tensor_train, input_tensor_val, target_tensor_train, target_tensor_val = train_test_split(input_tensor, target_tensor, test_size=0.2)\n",
    "\n",
    "# Show length\n",
    "print(len(input_tensor_train), len(target_tensor_train), len(input_tensor_val), len(target_tensor_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "976f6edb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert(lang, tensor):\n",
    "  for t in tensor:\n",
    "    if t!=0:\n",
    "      print (\"%d ----> %s\" % (t, lang.index_word[t]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3d49c2d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input Language; index to word mapping\n",
      "1 ----> <start>\n",
      "6 ----> том\n",
      "3295 ----> невиновен\n",
      "3 ----> .\n",
      "2 ----> <end>\n",
      "\n",
      "Target Language; index to word mapping\n",
      "1 ----> <start>\n",
      "76 ----> tom's\n",
      "1270 ----> innocent\n",
      "3 ----> .\n",
      "2 ----> <end>\n"
     ]
    }
   ],
   "source": [
    "print (\"Input Language; index to word mapping\")\n",
    "convert(inp_lang, input_tensor_train[0])\n",
    "print ()\n",
    "print (\"Target Language; index to word mapping\")\n",
    "convert(targ_lang, target_tensor_train[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "114b7838",
   "metadata": {},
   "source": [
    "#### Create a tf.data dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "89b49114",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-06-30 13:36:14.364807: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "BUFFER_SIZE = len(input_tensor_train)\n",
    "BATCH_SIZE = 64\n",
    "steps_per_epoch = len(input_tensor_train)//BATCH_SIZE\n",
    "embedding_dim = 300\n",
    "units = 1024\n",
    "vocab_inp_size = len(inp_lang.word_index)+1\n",
    "vocab_tar_size = len(targ_lang.word_index)+1\n",
    "\n",
    "dataset = tf.data.Dataset.from_tensor_slices((input_tensor_train, target_tensor_train)).shuffle(BUFFER_SIZE)\n",
    "dataset = dataset.batch(BATCH_SIZE, drop_remainder=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f339afcf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(TensorShape([64, 15]), TensorShape([64, 11]))"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example_input_batch, example_target_batch = next(iter(dataset))\n",
    "example_input_batch.shape, example_target_batch.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "62e4ff3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(tf.keras.Model):\n",
    "  def __init__(self, vocab_size, embedding_dim, enc_units, batch_sz):\n",
    "    super(Encoder, self).__init__()\n",
    "    self.batch_sz = batch_sz\n",
    "    self.enc_units = enc_units\n",
    "    self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
    "    self.gru = tf.keras.layers.GRU(self.enc_units,\n",
    "                                   return_sequences=False,\n",
    "                                   return_state=True,\n",
    "                                   recurrent_initializer='glorot_uniform')\n",
    "    \n",
    "\n",
    "  def call(self, x, hidden):\n",
    "    x = self.embedding(x)\n",
    "    output, state = self.gru(x, initial_state = hidden)\n",
    "    return state\n",
    "\n",
    "  def initialize_hidden_state(self):\n",
    "    return tf.zeros((self.batch_sz, self.enc_units))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "20f2a909",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoder Hidden state shape: (batch size, units) (64, 1024)\n"
     ]
    }
   ],
   "source": [
    "encoder = Encoder(vocab_inp_size, embedding_dim, units, BATCH_SIZE)\n",
    "\n",
    "# sample input\n",
    "sample_hidden = encoder.initialize_hidden_state()\n",
    "sample_hidden = encoder(example_input_batch, sample_hidden)\n",
    "# print ('Encoder output shape: (batch size, sequence length, units) {}'.format(sample_output.shape))\n",
    "print ('Encoder Hidden state shape: (batch size, units) {}'.format(sample_hidden.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "1316154f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(tf.keras.Model):\n",
    "  def __init__(self, vocab_size, embedding_dim, dec_units, batch_sz):\n",
    "    super(Decoder, self).__init__()\n",
    "    self.batch_sz = batch_sz\n",
    "    self.dec_units = dec_units\n",
    "    self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
    "    self.gru = tf.keras.layers.GRU(self.dec_units,\n",
    "                                   return_sequences=True,\n",
    "                                   return_state=True,\n",
    "                                   recurrent_initializer='glorot_uniform')\n",
    "    self.fc = tf.keras.layers.Dense(vocab_size)\n",
    "\n",
    "  def call(self, x, hidden):\n",
    "    # enc_output shape == (batch_size, max_length, hidden_size)\n",
    "\n",
    "    # x shape after passing through embedding == (batch_size, 1, embedding_dim)\n",
    "    x = self.embedding(x)\n",
    "\n",
    "    # x shape after concatenation == (batch_size, 1, embedding_dim + hidden_size)\n",
    "\n",
    "    # passing the concatenated vector to the GRU\n",
    "    output, state = self.gru(x, initial_state=hidden)\n",
    "\n",
    "    # output shape == (batch_size * 1, hidden_size)\n",
    "    output = tf.reshape(output, (-1, output.shape[2]))\n",
    "\n",
    "    # output shape == (batch_size, vocab)\n",
    "    # fc - fully connected слой\n",
    "    x = self.fc(output)\n",
    "\n",
    "    return x, state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "67598282",
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder = Decoder(vocab_tar_size, embedding_dim, units, BATCH_SIZE)\n",
    "\n",
    "decoder_sample_x, decoder_sample_h = decoder(tf.random.uniform((BATCH_SIZE, 1)),\n",
    "                                      sample_hidden)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "5f582631",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([64, 7334])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decoder_sample_x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "f13e8b5f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([64, 1024])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decoder_sample_h.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee2117dc",
   "metadata": {},
   "source": [
    "#### Define the optimizer and the loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "1de00b26",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = tf.keras.optimizers.Adam()\n",
    "\n",
    "loss_object = tf.keras.losses.SparseCategoricalCrossentropy(\n",
    "    from_logits=True, reduction='none')\n",
    "\n",
    "# это маскированный лосс. Мы не хотим чтобы паддиноговые нулевые токены учитывались в лоссе\n",
    "# поэтому мы получаем падинговые элементы (0) - маска\n",
    "# и зануляем там лосс\n",
    "def loss_function(real, pred):\n",
    "  mask = tf.math.logical_not(tf.math.equal(real, 0))\n",
    "  loss_ = loss_object(real, pred)\n",
    "\n",
    "  mask = tf.cast(mask, dtype=loss_.dtype)\n",
    "  loss_ *= mask\n",
    "\n",
    "  return tf.reduce_mean(loss_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3aa48f7",
   "metadata": {},
   "source": [
    "#### Checkpoints (Object-based saving)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "f7f500c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_dir = './training_nmt_checkpoints'\n",
    "\n",
    "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt\")\n",
    "\n",
    "checkpoint = tf.train.Checkpoint(optimizer=optimizer,\n",
    "                                 encoder=encoder,\n",
    "                                 decoder=decoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "55c9009a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# функция для обучения модели\n",
    "@tf.function\n",
    "def train_step(inp, targ, enc_hidden):\n",
    "    # inp - входная последовательность (русский)\n",
    "    # target - английский\n",
    "    # enc_hidden - вектор скрытого состояния энкодера\n",
    "    loss = 0\n",
    "    \n",
    "    # берём градиенты\n",
    "    with tf.GradientTape() as tape:\n",
    "        # передаём в энкодер входную последовательность и скрытое состояние\n",
    "        # получаем выход скрытого состояния\n",
    "        enc_hidden = encoder(inp, enc_hidden)\n",
    "        \n",
    "        # присваиваем для инициализации ветора скрытого состояния в декодере\n",
    "        dec_hidden = enc_hidden\n",
    "        # добавляем тег старта для начала перевода, берём его индекс, переводим в тензор\n",
    "        dec_input = tf.expand_dims([targ_lang.word_index['<start>']] * BATCH_SIZE, 1)\n",
    "\n",
    "        # Teacher forcing - feeding the target as the next input\n",
    "        # идём по таргету, по тому тексту на который хотим натренировать перевод\n",
    "        # начинаем не с нулевого токена, потому что первым стоит тег <start>\n",
    "        for t in range(1, targ.shape[1]):\n",
    "            # passing enc_output to the decoder\n",
    "            # мы не хотим затачиваться на предикт, прокидывая градиент будем улучшать предикт\n",
    "            predictions, dec_hidden = decoder(dec_input, dec_hidden)\n",
    "            # вычисляем лосс-функцию\n",
    "            loss += loss_function(targ[:, t], predictions)\n",
    "\n",
    "            # using teacher forcing\n",
    "            # записываем текущий токен (таргет) на вход декодера - получается смещение\n",
    "            dec_input = tf.expand_dims(targ[:, t], 1)\n",
    "\n",
    "        batch_loss = (loss / int(targ.shape[1]))\n",
    "        # формируем все переменные по которым будет браться градиент\n",
    "        variables = encoder.trainable_variables + decoder.trainable_variables\n",
    "        # берём градиент и применяем его к лосс-функции\n",
    "        gradients = tape.gradient(loss, variables)\n",
    "\n",
    "        optimizer.apply_gradients(zip(gradients, variables))\n",
    "    # возвращаем батч-лосс\n",
    "    return batch_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "9a496133",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-06-30 13:36:21.442404: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:176] None of the MLIR Optimization Passes are enabled (registered 2)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Batch 0 Loss 4.7538\n",
      "Epoch 1 Batch 100 Loss 2.0772\n",
      "Epoch 1 Batch 200 Loss 1.7587\n",
      "Epoch 1 Batch 300 Loss 1.6864\n",
      "Epoch 1 Batch 400 Loss 1.3951\n",
      "Epoch 1 Batch 500 Loss 1.3982\n",
      "Epoch 1 Batch 600 Loss 1.2116\n",
      "Epoch 1 Batch 700 Loss 1.1285\n",
      "Epoch 1 Batch 800 Loss 1.1628\n",
      "Epoch 1 Batch 900 Loss 1.0438\n",
      "Epoch 1 Batch 1000 Loss 1.0905\n",
      "Epoch 1 Batch 1100 Loss 1.1210\n",
      "Epoch 1 Batch 1200 Loss 0.9024\n",
      "Epoch 1 Loss 1.3719\n",
      "Time taken for 1 epoch 578.910238981247 sec\n",
      "\n",
      "Epoch 2 Batch 0 Loss 0.8467\n",
      "Epoch 2 Batch 100 Loss 0.7926\n",
      "Epoch 2 Batch 200 Loss 0.7120\n",
      "Epoch 2 Batch 300 Loss 0.6618\n",
      "Epoch 2 Batch 400 Loss 0.7225\n",
      "Epoch 2 Batch 500 Loss 0.6256\n",
      "Epoch 2 Batch 600 Loss 0.6533\n",
      "Epoch 2 Batch 700 Loss 0.5748\n",
      "Epoch 2 Batch 800 Loss 0.6634\n",
      "Epoch 2 Batch 900 Loss 0.5994\n",
      "Epoch 2 Batch 1000 Loss 0.5812\n",
      "Epoch 2 Batch 1100 Loss 0.5984\n",
      "Epoch 2 Batch 1200 Loss 0.5785\n",
      "Epoch 2 Loss 0.6483\n",
      "Time taken for 1 epoch 570.3650588989258 sec\n",
      "\n",
      "Epoch 3 Batch 0 Loss 0.3142\n",
      "Epoch 3 Batch 100 Loss 0.3617\n",
      "Epoch 3 Batch 200 Loss 0.4070\n",
      "Epoch 3 Batch 300 Loss 0.3867\n",
      "Epoch 3 Batch 400 Loss 0.3799\n",
      "Epoch 3 Batch 500 Loss 0.3100\n",
      "Epoch 3 Batch 600 Loss 0.2701\n",
      "Epoch 3 Batch 700 Loss 0.3222\n",
      "Epoch 3 Batch 800 Loss 0.3457\n",
      "Epoch 3 Batch 900 Loss 0.2816\n",
      "Epoch 3 Batch 1000 Loss 0.3650\n",
      "Epoch 3 Batch 1100 Loss 0.3079\n",
      "Epoch 3 Batch 1200 Loss 0.3069\n",
      "Epoch 3 Loss 0.3482\n",
      "Time taken for 1 epoch 571.6655678749084 sec\n",
      "\n",
      "Epoch 4 Batch 0 Loss 0.1768\n",
      "Epoch 4 Batch 100 Loss 0.2233\n",
      "Epoch 4 Batch 200 Loss 0.1517\n",
      "Epoch 4 Batch 300 Loss 0.2054\n",
      "Epoch 4 Batch 400 Loss 0.2508\n",
      "Epoch 4 Batch 500 Loss 0.1912\n",
      "Epoch 4 Batch 600 Loss 0.1799\n",
      "Epoch 4 Batch 700 Loss 0.2056\n",
      "Epoch 4 Batch 800 Loss 0.2403\n",
      "Epoch 4 Batch 900 Loss 0.1488\n",
      "Epoch 4 Batch 1000 Loss 0.2304\n",
      "Epoch 4 Batch 1100 Loss 0.3145\n",
      "Epoch 4 Batch 1200 Loss 0.1863\n",
      "Epoch 4 Loss 0.2096\n",
      "Time taken for 1 epoch 566.8324918746948 sec\n",
      "\n",
      "Epoch 5 Batch 0 Loss 0.1463\n",
      "Epoch 5 Batch 100 Loss 0.1180\n",
      "Epoch 5 Batch 200 Loss 0.1398\n",
      "Epoch 5 Batch 300 Loss 0.1010\n",
      "Epoch 5 Batch 400 Loss 0.1334\n",
      "Epoch 5 Batch 500 Loss 0.1356\n",
      "Epoch 5 Batch 600 Loss 0.1189\n",
      "Epoch 5 Batch 700 Loss 0.1658\n",
      "Epoch 5 Batch 800 Loss 0.1363\n",
      "Epoch 5 Batch 900 Loss 0.1710\n",
      "Epoch 5 Batch 1000 Loss 0.1634\n",
      "Epoch 5 Batch 1100 Loss 0.1330\n",
      "Epoch 5 Batch 1200 Loss 0.1576\n",
      "Epoch 5 Loss 0.1447\n",
      "Time taken for 1 epoch 579.9682967662811 sec\n",
      "\n"
     ]
    }
   ],
   "source": [
    "EPOCHS = 5\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "  start = time.time()\n",
    "\n",
    "  enc_hidden = encoder.initialize_hidden_state()\n",
    "  total_loss = 0\n",
    "\n",
    "  for (batch, (inp, targ)) in enumerate(dataset.take(steps_per_epoch)):\n",
    "    batch_loss = train_step(inp, targ, enc_hidden)\n",
    "    total_loss += batch_loss\n",
    "\n",
    "    if batch % 100 == 0:\n",
    "      print('Epoch {} Batch {} Loss {:.4f}'.format(epoch + 1,\n",
    "                                                   batch,\n",
    "                                                   batch_loss.numpy()))\n",
    "  # saving (checkpoint) the model every 2 epochs\n",
    "  if (epoch + 1) % 2 == 0:\n",
    "    checkpoint.save(file_prefix = checkpoint_prefix)\n",
    "\n",
    "  print('Epoch {} Loss {:.4f}'.format(epoch + 1,\n",
    "                                      total_loss / steps_per_epoch))\n",
    "  print('Time taken for 1 epoch {} sec\\n'.format(time.time() - start))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cf2e6ab",
   "metadata": {},
   "source": [
    "#### Translate\n",
    "\n",
    "* The evaluate function is similar to the training loop, except we don't use teacher forcing here. The input to the decoder at each time step is its previous predictions along with the hidden state and the encoder output.\n",
    "\n",
    "* Stop predicting when the model predicts the end token.\n",
    "\n",
    "* And store the attention weights for every time step.\n",
    "\n",
    "\n",
    "Note: The encoder output is calculated only once for one input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "c9e396c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# функция для оценки перевода\n",
    "def evaluate(sentence):\n",
    "    attention_plot = np.zeros((max_length_targ, max_length_inp))\n",
    "    # препроцессинг предложения\n",
    "    sentence = preprocess_sentence(sentence)\n",
    "    # переводим каждый токен в индекс\n",
    "    inputs = [inp_lang.word_index[i] for i in sentence.split(' ')]\n",
    "    inputs = tf.keras.preprocessing.sequence.pad_sequences([inputs],\n",
    "                                                         maxlen=max_length_inp,\n",
    "                                                         padding='post')\n",
    "    # переводим в тензор\n",
    "    inputs = tf.convert_to_tensor(inputs)\n",
    "\n",
    "    result = ''\n",
    "\n",
    "    hidden = [tf.zeros((1, units))]\n",
    "    # посылаем в энкодер\n",
    "    enc_hidden = encoder(inputs, hidden)\n",
    "\n",
    "    dec_hidden = enc_hidden\n",
    "    dec_input = tf.expand_dims([targ_lang.word_index['<start>']], 0)\n",
    "\n",
    "    for t in range(max_length_targ):\n",
    "        predictions, dec_hidden = decoder(dec_input, dec_hidden)\n",
    "\n",
    "        # storing the attention weights to plot later on\n",
    "        predicted_id = tf.argmax(predictions[0]).numpy()\n",
    "        # т.к. у нас нет таргета - передаём предсказание на следующий шаг\n",
    "        result += targ_lang.index_word[predicted_id] + ' '\n",
    "        # останавливаемся если встречаем токен конца\n",
    "        if targ_lang.index_word[predicted_id] == '<end>':\n",
    "            return result, sentence\n",
    "\n",
    "    # the predicted ID is fed back into the model\n",
    "    dec_input = tf.expand_dims([predicted_id], 0)\n",
    "\n",
    "    return result, sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "f8939570",
   "metadata": {},
   "outputs": [],
   "source": [
    "def translate(sentence):\n",
    "  result, sentence = evaluate(sentence)\n",
    "\n",
    "  print('Input: %s' % (sentence))\n",
    "  print('Predicted translation: {}'.format(result))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fef27371",
   "metadata": {},
   "source": [
    "#### Restore the latest checkpoint and test\n",
    "\n",
    "Так как нет механизма внимания, при увеличении длины предложения качество падает."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "952fe00f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.training.tracking.util.CheckpointLoadStatus at 0x7fc6be8a4910>"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# restoring the latest checkpoint in checkpoint_dir\n",
    "checkpoint.restore(tf.train.latest_checkpoint(checkpoint_dir))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "7b5034a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: <start> как хочется чтобы учиться было легко <end>\n",
      "Predicted translation: how can to ? ! ! ! ! ! ? ? \n"
     ]
    }
   ],
   "source": [
    "translate('Как хочется чтобы учиться было легко')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "71ecd538",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: <start> в этом году снова без отпуска осталась <end>\n",
      "Predicted translation: is this in this ! ! ! ! ! ! ! \n"
     ]
    }
   ],
   "source": [
    "translate('В этом году снова без отпуска осталась')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "0816a204",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: <start> бывает ли лучше чем сейчас ? <end>\n",
      "Predicted translation: how how what , ? ? ? ? ? ? ? \n"
     ]
    }
   ],
   "source": [
    "translate('Бывает ли лучше чем сейчас?')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "5fb60e5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: <start> прекрасное далеко ? <end>\n",
      "Predicted translation: is the drunk ? ? ? ! ! ! ! ! \n"
     ]
    }
   ],
   "source": [
    "translate('Прекрасное далеко?')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "4460ed67",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: <start> как прекрасен этот мир ! посмотри и улыбнись ! <end>\n",
      "Predicted translation: how all so ! ! ! ! ! ! ! . \n"
     ]
    }
   ],
   "source": [
    "translate('Как прекрасен этот мир! Посмотри и улыбнись!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "5e7ab660",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: <start> чтоб я так жил ! <end>\n",
      "Predicted translation: do me quickly fast . . . . . . . \n"
     ]
    }
   ],
   "source": [
    "translate('Чтоб я так жил!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "f43b863e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: <start> и дн м , и ночью кот все ходит по цепи . <end>\n",
      "Predicted translation: seeing if , , ! . . . . . . \n"
     ]
    }
   ],
   "source": [
    "translate('И днём, и ночью кот все ходит по цепи.') # ученый, кругом - не распознает"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "be37786e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: <start> золотая на груди . <end>\n",
      "Predicted translation: the the and . . . . . . . . \n"
     ]
    }
   ],
   "source": [
    "translate('золотая на груди.') # Ночевала, тучка, утеса, великана - не распознает"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "1333cda7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: <start> поднимается медленно в гору . . . <end>\n",
      "Predicted translation: live finally , . . . . . . . . \n"
     ]
    }
   ],
   "source": [
    "translate('поднимается медленно в гору...') # Гляжу - не распознает"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "5841e6c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: <start> он себя заставил и лучше не мог . <end>\n",
      "Predicted translation: he he all he . . . . . . . \n"
     ]
    }
   ],
   "source": [
    "translate('Он себя заставил и лучше не мог.') # уважать, выдумать - не распознает"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2975f730",
   "metadata": {},
   "source": [
    "Не стала продолжать примеры из-за отвратительного перевода. Скорее всего из-за маленькой величины эпох. Маленькое количество эпох выбрала в целях экономии времени на обучение."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa244383",
   "metadata": {},
   "outputs": [],
   "source": [
    "translate('Мороз и солнце - день чудесный')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a88cb05",
   "metadata": {},
   "outputs": [],
   "source": [
    "translate('Итак, звалась она Татьяна...')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1cf7ee4",
   "metadata": {},
   "outputs": [],
   "source": [
    "translate('Все смешалось в доме Облонских')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd4f5ea6",
   "metadata": {},
   "outputs": [],
   "source": [
    "translate('Понимаешь, все еще будет...')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7b40cef",
   "metadata": {},
   "outputs": [],
   "source": [
    "translate('На ель ворона взгромоздясь, позавтракать совсем уж было собралась')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "894006e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "translate('Тучки небесные, вечные странники')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02cd16b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "translate('Расцветали яблони и груши')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68ce1000",
   "metadata": {},
   "outputs": [],
   "source": [
    "translate('По усам хотя бежало, в рот ни капли не попало')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32805f11",
   "metadata": {},
   "outputs": [],
   "source": [
    "translate('Победа будет за нами')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
